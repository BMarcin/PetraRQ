{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa2ba595-106f-4cf8-94af-8e359905b67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb38c629-8a6c-48c4-9004-74a15c213cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import wandb\n",
    "from torch import nn\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import OneCycleLR, CyclicLR\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "833e8222-2f9a-41a0-97a6-c8aafc2680b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f7e76b6-9b9b-4dbb-b9ad-8a9b324c8ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor, TQDMProgressBar, StochasticWeightAveraging\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "import logging\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from src.PetraRQ.PetraRQDatasets import LanguageModellingDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a058c852-a8a4-4404-980f-92726856f0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import seed_everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83ca7611-1395-49c2-b2cc-db0b2027fcb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_everything(1, workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a50c95b2-c264-4d18-b5f5-20bc66a6c4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    format='%(asctime)s - %(levelname)s - %(name)s -  %(message)s',\n",
    "    level=logging.INFO\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb66c2b7-4dee-450c-b808-fb0ca64e1cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"../../data/train/lm.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     train_data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfa11eda-fcba-4921-b1f2-47c0cac5a24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../data/dev/lm.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    dev_data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01142302-83c5-4718-8dd8-5c2e42485057",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 6\n",
    "dev_batch_size = 6\n",
    "shuffle = False\n",
    "steps = 4000\n",
    "d_model = 512\n",
    "num_tokens = 16000\n",
    "seq_length = 256\n",
    "overlapping_part = 64\n",
    "depth = 6\n",
    "# k = 256\n",
    "heads = 8\n",
    "# dim_head = None\n",
    "# one_kv_head = False\n",
    "# share_kv = True\n",
    "dropout = 0.1\n",
    "optimizer = \"adagrad\"\n",
    "lr_min=1e-5\n",
    "lr_max=3e-4\n",
    "accumulate_grad_batches=2\n",
    "duplicate_dataset_ratio=1\n",
    "inputs_masking = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b61894b3-459b-4ad1-b456-8eefe8fa0f90",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "356c5b9f2e9f42dbb3422a7b912a8404",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Hash training data:   0%|          | 0/5003 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-13 00:02:15,215 - INFO - root -  Training data hash: 3ded16b3953922cceaa90923883a7147\n",
      "2022-08-13 00:02:15,216 - INFO - root -  Loading tokenizer from disk\n",
      "2022-08-13 00:02:15,238 - INFO - root -  LMDS dev_dataset: 5003\n",
      "2022-08-13 00:02:15,241 - INFO - root -  LMDS train_dataset: 5003\n"
     ]
    }
   ],
   "source": [
    "lm_ds = LanguageModellingDataset(\n",
    "    train_data=dev_data,\n",
    "    test_data=None,\n",
    "    dev_data=dev_data,\n",
    "    vocab_size=num_tokens,\n",
    "    # max_len=seq_length,\n",
    "    # batch_size=train_batch_size,\n",
    "    duplicate_dataset_ratio=duplicate_dataset_ratio,\n",
    "    masking_token_parts=inputs_masking,\n",
    "    use_incremental_samples=True,\n",
    "    incremental_samples_min=50,\n",
    "    incremental_samples_step=30,\n",
    "    increment_every_x_steps=600\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17628537-7b07-4c38-a856-998a87e6e7fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_ds.tokenizer.token_to_id(\"[PAD]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b520f394-f243-4de5-a3ae-c64aaaab540f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coll_fn(batch):\n",
    "    texts = []\n",
    "    labels = []\n",
    "    incremental_indexes = []\n",
    "    \n",
    "    pad_token = 2\n",
    "    \n",
    "    # print(len(batch[0]))\n",
    "    if len(batch[0]) == 3:\n",
    "        for (text, label, incremental_index) in batch:\n",
    "            texts.append(torch.tensor(text).to(\"cpu\"))\n",
    "            labels.append(torch.tensor(label).to(\"cpu\"))\n",
    "            incremental_indexes.append(incremental_index)\n",
    "    else:\n",
    "        for (text, label) in batch:\n",
    "            texts.append(torch.tensor(text).to(\"cpu\"))\n",
    "            labels.append(torch.tensor(label).to(\"cpu\"))\n",
    "        \n",
    "    ins = nn.utils.rnn.pad_sequence(texts, batch_first=True, padding_value=pad_token)\n",
    "    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "    \n",
    "    if len(incremental_indexes) > 0:\n",
    "        return ins.numpy(), labels.numpy(), np.array(incremental_indexes)\n",
    "    else:\n",
    "        return ins.numpy(), labels.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d194bc8-bf7e-4ad1-aa7f-e9bef83faef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = DataLoader(\n",
    "    lm_ds.train_dataset,\n",
    "    batch_size=train_batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=False,\n",
    "    collate_fn=coll_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85fb2daa-dfef-41ca-ba4d-0e1bb8edb9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data_loader = DataLoader(\n",
    "    lm_ds.dev_dataset,\n",
    "    batch_size=dev_batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=False,\n",
    "    collate_fn=coll_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb2a796f-1e52-40bd-9f4e-fadb4333d5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 50000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "154984c8-78a1-4213-8073-0da9ee8a79de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PetraRQ(pl.LightningModule):\n",
    "    def __init__(\n",
    "            self,\n",
    "            d_model,\n",
    "            num_tokens,\n",
    "            seq_length,\n",
    "            overlapping_part,\n",
    "            depth,\n",
    "            heads=8,\n",
    "            dropout=0.1,\n",
    "            steps=1000,\n",
    "            lr_min=1e-4,\n",
    "            lr_max=3e-3,\n",
    "            optim=\"adam\"\n",
    "    ):\n",
    "        super(PetraRQ, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_tokens = num_tokens\n",
    "        self.seq_length = seq_length\n",
    "        self.overlapping_part = overlapping_part\n",
    "        self.depth = depth\n",
    "        self.heads = heads\n",
    "        self.dropout = dropout\n",
    "        self.steps = steps\n",
    "        self.lr_min = lr_min\n",
    "        self.lr_max = lr_max\n",
    "        self.optim = optim\n",
    "        self.overlapping_part = overlapping_part\n",
    "        self.activation = nn.GELU()\n",
    "        self.out_norm = nn.LayerNorm(num_tokens)\n",
    "        self.memory_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "        assert (self.optim == 'adam' or self.optim == 'adagrad'), 'Optim must be set to \"adam\" or \"adagrad\"'\n",
    "\n",
    "        self.token_emb = nn.Embedding(num_tokens, d_model)\n",
    "        self.pos_emb = PositionalEncoding(d_model)\n",
    "        \n",
    "        self.former_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=heads)\n",
    "        self.former = nn.TransformerEncoder(self.former_layer, num_layers=depth)\n",
    "        \n",
    "        self.to_logits = nn.Linear(d_model, num_tokens)\n",
    "\n",
    "\n",
    "    def forward(self, x_in):\n",
    "        floating_memory = None\n",
    "        output_hidden_layers = None\n",
    "        floating_mems = None\n",
    "        \n",
    "        i = 0\n",
    "        while ((output_hidden_layers is None) or (output_hidden_layers.shape[1] + (x_in.shape[1] % self.overlapping_part) < x_in.shape[1])):\n",
    "        \n",
    "            if floating_memory is None:\n",
    "                x = self.token_emb(x_in[:, :self.seq_length].to(self.device))\n",
    "                x_pos = self.pos_emb(x)\n",
    "            else:\n",
    "                toks = x_in[:, int((self.overlapping_part * (i+3))):int((self.overlapping_part * (i+4)))].to(self.device)\n",
    "                embeds = self.token_emb(toks)\n",
    "                x_pos = self.pos_emb(embeds)\n",
    "                x_pos = torch.cat((x[:, self.overlapping_part:, :], x_pos), dim=1)\n",
    "                \n",
    "            x = self.former(x_pos)\n",
    "            \n",
    "            if floating_memory is None:\n",
    "                floating_memory = x[:, :self.overlapping_part, :]\n",
    "                output_hidden_layers = x[:, :self.overlapping_part, :]\n",
    "                # floating_mems = x_pos[:, :self.overlapping_part, :]\n",
    "            else:\n",
    "                add = floating_memory + x[:, :self.overlapping_part, :]\n",
    "                floating_memory = self.memory_norm(add)\n",
    "                \n",
    "                output_hidden_layers = torch.cat((output_hidden_layers, x[:, :self.overlapping_part, :]), dim=1)\n",
    "                # floating_mems = torch.cat((floating_mems, floating_memory), dim=1)\n",
    "            i += 1\n",
    "\n",
    "        output_hidden_layers = torch.cat((output_hidden_layers, x[:, self.overlapping_part:, :]), dim=1)\n",
    "        out = self.to_logits(output_hidden_layers)\n",
    "        return out\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        if self.optim == 'adagrad':\n",
    "            optimizer = torch.optim.Adagrad(\n",
    "                self.parameters(),\n",
    "                lr=self.lr_min,\n",
    "                weight_decay=0.01\n",
    "            )\n",
    "        elif self.optim == 'adam':\n",
    "            optimizer = torch.optim.AdamW(\n",
    "                self.parameters(),\n",
    "                lr=self.lr_min,\n",
    "                weight_decay=0.01,\n",
    "                betas=(0.9, 0.999)\n",
    "            )\n",
    "\n",
    "        lr_scheduler = OneCycleLR(\n",
    "            optimizer,\n",
    "            max_lr=self.lr_max,\n",
    "            total_steps=self.steps,\n",
    "            cycle_momentum=False,\n",
    "        )\n",
    "\n",
    "        return [optimizer], [{'scheduler': lr_scheduler, 'interval': 'step'}]\n",
    "        # return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # x, y, token_pos = batch\n",
    "        xs, ys, incremental_indexs = batch\n",
    "        \n",
    "        x = torch.tensor(xs).to(\"cpu\")\n",
    "        y = torch.tensor(ys).to(self.device)\n",
    "        incremental_index = torch.tensor(incremental_indexs).to(self.device)\n",
    "        # print(incremental_index)\n",
    "        x = self.forward(x)\n",
    "        # print('x shape', x.shape)\n",
    "        # print('y shape', y.shape)\n",
    "        # print(token_pos)\n",
    "        # print('output shape', x.shape)\n",
    "        # print('target shape', y.shape)\n",
    "        # print('cross entropy x', x.view(-1, self.num_tokens).shape)\n",
    "        # print('cross entropy y', y.long().view(-1).shape)\n",
    "\n",
    "        # loss = F.cross_entropy(x[range(x.shape[0]), token_pos, :], y)\n",
    "        # print('view', x.view(-1, self.num_tokens).shape, y.long().view(-1).shape)\n",
    "        loss = F.cross_entropy(x.view(-1, self.num_tokens), y.long().view(-1))\n",
    "        self.log('train/loss', loss, prog_bar=True, batch_size=x.shape[0])\n",
    "        self.log('padding_mask', torch.mean(incremental_index.float()), prog_bar=True, batch_size=x.shape[0])\n",
    "        wandb.log({'train/loss': loss})\n",
    "\n",
    "        perplexity = torch.exp(loss)\n",
    "        self.log('train/perplexity', perplexity, prog_bar=True, batch_size=x.shape[0])\n",
    "        wandb.log({'train/perplexity': perplexity})\n",
    "\n",
    "        # wandb.log({'train/learning_rate': self.optimizers[0].param_groups[0]['lr']})\n",
    "\n",
    "        return {'loss': loss, 'perplexity': perplexity}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # print(batch)\n",
    "        # x, y, token_pos = batch\n",
    "        xs, ys = batch\n",
    "        x = torch.tensor(xs).to(\"cpu\")\n",
    "        y = torch.tensor(ys).to(self.device)\n",
    "        \n",
    "        # print(x, y)\n",
    "        # print('x', x.shape)\n",
    "        # print('y', y.shape)\n",
    "        x = self.forward(x)\n",
    "        # loss = F.cross_entropy(x[range(x.shape[0]), token_pos, :], y)\n",
    "        # print('view', x.view(-1, self.num_tokens).shape, y.long().view(-1).shape)\n",
    "        loss = F.cross_entropy(x.view(-1, self.num_tokens), y.long().view(-1))\n",
    "        # print('loss', loss)\n",
    "        perplexity = torch.exp(loss)\n",
    "        self.log('eval/loss', loss, prog_bar=True, batch_size=x.shape[0])\n",
    "        wandb.log({'eval/loss': loss})\n",
    "        self.log('eval/perplexity', perplexity, prog_bar=True, batch_size=x.shape[0])\n",
    "        wandb.log({'eval/perplexity': perplexity})\n",
    "        return {'val_loss': loss, 'val_perplexity': perplexity}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e42dfe42-6d42-4ea7-b49a-793803b23e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "petra = PetraRQ(\n",
    "    d_model=d_model,\n",
    "    num_tokens=num_tokens,\n",
    "    seq_length=seq_length,\n",
    "    overlapping_part=overlapping_part,\n",
    "    depth=depth,\n",
    "    heads=heads,\n",
    "    dropout=dropout,\n",
    "    steps=steps,\n",
    "    optim=optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22814f49-d52c-41c7-bbc4-2cf8ff5acf37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-13 00:02:22,739 - ERROR - wandb.jupyter -  Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "wandb: Currently logged in as: bmarcin. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Praca\\PetraRQ2\\PetraRQ\\notebooks\\PetraRQ\\wandb\\run-20220813_000225-i70mh4ko</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/bmarcin/PetraRQ/runs/i70mh4ko\" target=\"_blank\">PetraRQ_transformer_recurrent_test</a></strong> to <a href=\"https://wandb.ai/bmarcin/PetraRQ\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb_logger = WandbLogger(\n",
    "    project=\"PetraRQ\",\n",
    "    name=\"PetraRQ_transformer_recurrent_test\",\n",
    "    log_model=\"all\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c634e7ec-8dbc-46fe-a431-c0373e1cae2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_logger.experiment.config['batch_size'] = train_batch_size\n",
    "wandb_logger.experiment.config['steps'] = steps\n",
    "wandb_logger.experiment.config['d_model'] = d_model\n",
    "wandb_logger.experiment.config['num_tokens'] = num_tokens\n",
    "wandb_logger.experiment.config['seq_length'] = seq_length\n",
    "wandb_logger.experiment.config['depth'] = depth\n",
    "wandb_logger.experiment.config['heads'] = heads\n",
    "wandb_logger.experiment.config['dropout'] = dropout\n",
    "wandb_logger.experiment.config['optimizer'] = optimizer\n",
    "wandb_logger.experiment.config['duplicate_dataset_ratio'] = duplicate_dataset_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "afcfc6b5-dd19-4e15-b903-45e4cded1207",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    devices=1,\n",
    "    max_steps=steps,\n",
    "    log_every_n_steps=5,\n",
    "    accelerator='gpu',\n",
    "    accumulate_grad_batches=accumulate_grad_batches,\n",
    "    val_check_interval=0.05,\n",
    "    # val_check_interval=300,\n",
    "    default_root_dir='./PetraRQmodel',\n",
    "    enable_checkpointing=False,\n",
    "    callbacks=[\n",
    "        # ModelCheckpoint(\n",
    "        #     dirpath='./PetraRQmodel/checkpoints',\n",
    "        #     save_top_k=3,\n",
    "        #     monitor='eval/loss',\n",
    "        #     mode='min',\n",
    "        #     filename='petrarq-{epoch}-{val_loss:.2f}.ckpt'\n",
    "        # ),\n",
    "        EarlyStopping(\n",
    "            monitor='train/loss',\n",
    "            mode='min',\n",
    "            patience=6,\n",
    "            check_finite=True,\n",
    "        ),\n",
    "        LearningRateMonitor(logging_interval='step'),\n",
    "        TQDMProgressBar(refresh_rate=1),\n",
    "        # StochasticWeightAveraging(swa_lrs=1e-3)\n",
    "    ],\n",
    "    logger=wandb_logger,\n",
    "    reload_dataloaders_every_n_epochs=0,\n",
    "    gradient_clip_val=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "51273443-3d2c-4c2d-b046-c1d731669e64",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "D:\\PetraRQ\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12f075af5ddf414788c1901fd3e03aed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-13 00:02:38,789 - INFO - root -  Incrementing epochs done to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incrementing epochs done to 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-13 00:02:55,688 - INFO - root -  Incrementing epochs done to 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incrementing epochs done to 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-13 00:03:20,420 - INFO - root -  Incrementing epochs done to 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incrementing epochs done to 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-13 00:04:00,722 - INFO - root -  Incrementing epochs done to 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incrementing epochs done to 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-13 00:05:15,457 - INFO - root -  Incrementing epochs done to 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incrementing epochs done to 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-13 00:07:16,425 - INFO - root -  Incrementing epochs done to 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incrementing epochs done to 6\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 3.20 GiB (GPU 0; 8.00 GiB total capacity; 3.44 GiB already allocated; 2.93 GiB free; 3.46 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpetra\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdev_data_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\PetraRQ\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:850\u001b[0m, in \u001b[0;36mTrainer.validate\u001b[1;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[0;32m    825\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;124;03mPerform one evaluation epoch over the validation set.\u001b[39;00m\n\u001b[0;32m    827\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    847\u001b[0m \u001b[38;5;124;03m    The length of the list corresponds to the number of validation dataloaders used.\u001b[39;00m\n\u001b[0;32m    848\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m model \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module\n\u001b[1;32m--> 850\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\PetraRQ\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:723\u001b[0m, in \u001b[0;36mTrainer._call_and_handle_interrupt\u001b[1;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    722\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 723\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    724\u001b[0m \u001b[38;5;66;03m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001b[39;00m\n\u001b[0;32m    725\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n",
      "File \u001b[1;32mD:\\PetraRQ\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:897\u001b[0m, in \u001b[0;36mTrainer._validate_impl\u001b[1;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validated_ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mckpt_path  \u001b[38;5;66;03m# TODO: remove in v1.8\u001b[39;00m\n\u001b[0;32m    896\u001b[0m \u001b[38;5;66;03m# run validate\u001b[39;00m\n\u001b[1;32m--> 897\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    899\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mD:\\PetraRQ\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1236\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m   1232\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39mrestore_training_state()\n\u001b[0;32m   1234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39mresume_end()\n\u001b[1;32m-> 1236\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1238\u001b[0m log\u001b[38;5;241m.\u001b[39mdetail(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1239\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_teardown()\n",
      "File \u001b[1;32mD:\\PetraRQ\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1320\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1317\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mdispatch(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluating:\n\u001b[1;32m-> 1320\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredicting:\n\u001b[0;32m   1322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_predict()\n",
      "File \u001b[1;32mD:\\PetraRQ\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1365\u001b[0m, in \u001b[0;36mTrainer._run_evaluate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1362\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evaluation_loop\u001b[38;5;241m.\u001b[39mtrainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m   1364\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_evaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m), torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m-> 1365\u001b[0m     eval_loop_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1367\u001b[0m \u001b[38;5;66;03m# remove the tensors from the eval results\u001b[39;00m\n\u001b[0;32m   1368\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m eval_loop_results:\n",
      "File \u001b[1;32mD:\\PetraRQ\\lib\\site-packages\\pytorch_lightning\\loops\\base.py:204\u001b[0m, in \u001b[0;36mLoop.run\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madvance(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mD:\\PetraRQ\\lib\\site-packages\\pytorch_lightning\\loops\\dataloader\\evaluation_loop.py:155\u001b[0m, in \u001b[0;36mEvaluationLoop.advance\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_dataloaders \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    154\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataloader_idx\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m dataloader_idx\n\u001b[1;32m--> 155\u001b[0m dl_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdl_max_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;66;03m# store batch level output per dataloader\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs\u001b[38;5;241m.\u001b[39mappend(dl_outputs)\n",
      "File \u001b[1;32mD:\\PetraRQ\\lib\\site-packages\\pytorch_lightning\\loops\\base.py:204\u001b[0m, in \u001b[0;36mLoop.run\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madvance(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mD:\\PetraRQ\\lib\\site-packages\\pytorch_lightning\\loops\\epoch\\evaluation_epoch_loop.py:128\u001b[0m, in \u001b[0;36mEvaluationEpochLoop.advance\u001b[1;34m(self, data_fetcher, dl_max_batches, kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_started()\n\u001b[0;32m    127\u001b[0m \u001b[38;5;66;03m# lightning module methods\u001b[39;00m\n\u001b[1;32m--> 128\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evaluation_step(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    129\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evaluation_step_end(output)\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n",
      "File \u001b[1;32mD:\\PetraRQ\\lib\\site-packages\\pytorch_lightning\\loops\\epoch\\evaluation_epoch_loop.py:226\u001b[0m, in \u001b[0;36mEvaluationEpochLoop._evaluation_step\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    224\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39m_call_strategy_hook(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[0;32m    225\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 226\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalidation_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mD:\\PetraRQ\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1765\u001b[0m, in \u001b[0;36mTrainer._call_strategy_hook\u001b[1;34m(self, hook_name, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   1764\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1765\u001b[0m     output \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m   1768\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32mD:\\PetraRQ\\lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:344\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;124;03m\"\"\"The actual validation step.\u001b[39;00m\n\u001b[0;32m    340\u001b[0m \n\u001b[0;32m    341\u001b[0m \u001b[38;5;124;03mSee :meth:`~pytorch_lightning.core.lightning.LightningModule.validation_step` for more details\u001b[39;00m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin\u001b[38;5;241m.\u001b[39mval_step_context():\n\u001b[1;32m--> 344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mvalidation_step(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36mPetraRQ.validation_step\u001b[1;34m(self, batch, batch_idx)\u001b[0m\n\u001b[0;32m    147\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(x)\n\u001b[0;32m    148\u001b[0m \u001b[38;5;66;03m# loss = F.cross_entropy(x[range(x.shape[0]), token_pos, :], y)\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;66;03m# print('view', x.view(-1, self.num_tokens).shape, y.long().view(-1).shape)\u001b[39;00m\n\u001b[1;32m--> 150\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_tokens\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;66;03m# print('loss', loss)\u001b[39;00m\n\u001b[0;32m    152\u001b[0m perplexity \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(loss)\n",
      "File \u001b[1;32mD:\\PetraRQ\\lib\\site-packages\\torch\\nn\\functional.py:3014\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3012\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3013\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3014\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 3.20 GiB (GPU 0; 8.00 GiB total capacity; 3.44 GiB already allocated; 2.93 GiB free; 3.46 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "trainer.validate(petra, dev_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1258080e-1203-4680-9784-01891d2bcbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(petra, train_data_loader, dev_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd42969c-d6cb-4355-b9f7-c9fcb2084c82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
