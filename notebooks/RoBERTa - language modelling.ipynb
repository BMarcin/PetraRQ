{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RoBERTa LM training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "wandb: Currently logged in as: bmarcin (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 16_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = [\n",
    "    '<url>',\n",
    "    '<email>',\n",
    "    '<number>',\n",
    "    '<date>', \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_ds = \"../data/dev/lm.txt\"\n",
    "test_ds = \"../data/test/lm.txt\"\n",
    "train_ds = \"../data/train/lm.txt\"\n",
    "\n",
    "notebook_path_prefix = \"roberta_lm\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe = ByteLevelBPETokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe.train(\n",
    "    files=[train_ds], \n",
    "    vocab_size=vocab_size, \n",
    "    min_frequency=2, \n",
    "    special_tokens=[\n",
    "        \"<s>\",\n",
    "        \"<pad>\",\n",
    "        \"</s>\",\n",
    "        \"<unk>\",\n",
    "        \"<mask>\",\n",
    "    ] + special_tokens\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['roberta_lm\\\\vocab.json', 'roberta_lm\\\\merges.txt']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.makedirs(notebook_path_prefix, exist_ok=True)\n",
    "bpe.save_model(notebook_path_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig, RobertaTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "file roberta_lm\\config.json not found\n",
      "file roberta_lm\\config.json not found\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizerFast.from_pretrained(notebook_path_prefix, max_len=512, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_special_tokens({\n",
    "    'additional_special_tokens': special_tokens\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<s>',\n",
       " 'eos_token': '</s>',\n",
       " 'unk_token': '<unk>',\n",
       " 'sep_token': '</s>',\n",
       " 'pad_token': '<pad>',\n",
       " 'cls_token': '<s>',\n",
       " 'mask_token': '<mask>',\n",
       " 'additional_special_tokens': ['<url>', '<email>', '<number>', '<date>']}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-abbb67285606e5bb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset text/default to C:\\Users\\Marcin Borzymowski\\.cache\\huggingface\\datasets\\text\\default-abbb67285606e5bb\\0.0.0\\e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40cbe293402c4110b943024e2cdf142a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e27522984324912b1c5d522a982146e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset text downloaded and prepared to C:\\Users\\Marcin Borzymowski\\.cache\\huggingface\\datasets\\text\\default-abbb67285606e5bb\\0.0.0\\e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f259079a34064167ae903eff6fcca261",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset('text', data_files={'train': [train_ds], 'test': [test_ds], 'dev': [dev_ds]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6903b630e7c343ed9ef291ca5bee982c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb63d352a6cf4b4a84cca866012116fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb44918680ff47feba6ddbcb870abeb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = dataset.map(\n",
    "    encode,\n",
    "    batched=True,\n",
    "    remove_columns=['text'],\n",
    "    load_from_cache_file=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DS collocator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = RobertaConfig(\n",
    "    vocab_size=vocab_size,\n",
    "    max_position_embeddings=514,\n",
    "    num_attention_heads=8,\n",
    "    num_hidden_layers=6,\n",
    "    type_vocab_size=1,\n",
    "    layer_norm_eps=0.00001,\n",
    "    hidden_size=512,\n",
    "    hidden_dropout_prob=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RobertaForMaskedLM(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33948288"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=notebook_path_prefix+\"_lm\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=180,\n",
    "    per_device_train_batch_size=12,\n",
    "    per_device_eval_batch_size=24,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=3,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    no_cuda=False,\n",
    "    logging_steps=2500,\n",
    "    eval_steps=2500,\n",
    "    evaluation_strategy='steps',\n",
    "    report_to=\"wandb\",\n",
    "    run_name=\"roberta-lm\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets['dev']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 23999\n",
      "  Num Epochs = 180\n",
      "  Instantaneous batch size per device = 12\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 360000\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "wandb: wandb version 0.12.6 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/bmarcin/huggingface/runs/3an3b5mm\" target=\"_blank\">roberta-lm</a></strong> to <a href=\"https://wandb.ai/bmarcin/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='234877' max='360000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [234877/360000 18:46:31 < 10:00:07, 3.47 it/s, Epoch 117.44/180]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>5.826200</td>\n",
       "      <td>5.270166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>5.171000</td>\n",
       "      <td>4.771377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>4.573300</td>\n",
       "      <td>3.736655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>3.796100</td>\n",
       "      <td>3.150887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>3.327700</td>\n",
       "      <td>2.743751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>2.944700</td>\n",
       "      <td>2.442991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>2.643500</td>\n",
       "      <td>2.219984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>2.426100</td>\n",
       "      <td>2.053136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>2.239800</td>\n",
       "      <td>1.896559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>2.073300</td>\n",
       "      <td>1.741439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>1.918300</td>\n",
       "      <td>1.617314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>1.797600</td>\n",
       "      <td>1.528647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>1.705700</td>\n",
       "      <td>1.462922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>1.631100</td>\n",
       "      <td>1.410821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37500</td>\n",
       "      <td>1.569200</td>\n",
       "      <td>1.349712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>1.517300</td>\n",
       "      <td>1.314929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42500</td>\n",
       "      <td>1.462900</td>\n",
       "      <td>1.285961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>1.431400</td>\n",
       "      <td>1.250647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47500</td>\n",
       "      <td>1.389900</td>\n",
       "      <td>1.229545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>1.358600</td>\n",
       "      <td>1.196149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52500</td>\n",
       "      <td>1.329300</td>\n",
       "      <td>1.188103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55000</td>\n",
       "      <td>1.297400</td>\n",
       "      <td>1.164936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57500</td>\n",
       "      <td>1.272500</td>\n",
       "      <td>1.141916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>1.252000</td>\n",
       "      <td>1.125433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62500</td>\n",
       "      <td>1.230100</td>\n",
       "      <td>1.103727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65000</td>\n",
       "      <td>1.209600</td>\n",
       "      <td>1.094797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67500</td>\n",
       "      <td>1.187400</td>\n",
       "      <td>1.079717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>1.165800</td>\n",
       "      <td>1.076602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72500</td>\n",
       "      <td>1.151700</td>\n",
       "      <td>1.052840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75000</td>\n",
       "      <td>1.134700</td>\n",
       "      <td>1.042712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77500</td>\n",
       "      <td>1.124400</td>\n",
       "      <td>1.027194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80000</td>\n",
       "      <td>1.105100</td>\n",
       "      <td>1.019123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82500</td>\n",
       "      <td>1.093900</td>\n",
       "      <td>1.013864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85000</td>\n",
       "      <td>1.078800</td>\n",
       "      <td>1.011686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87500</td>\n",
       "      <td>1.065900</td>\n",
       "      <td>0.990414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90000</td>\n",
       "      <td>1.054600</td>\n",
       "      <td>0.982556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92500</td>\n",
       "      <td>1.039200</td>\n",
       "      <td>0.972881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95000</td>\n",
       "      <td>1.038600</td>\n",
       "      <td>0.975718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97500</td>\n",
       "      <td>1.021800</td>\n",
       "      <td>0.963799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100000</td>\n",
       "      <td>1.011000</td>\n",
       "      <td>0.949469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102500</td>\n",
       "      <td>1.003600</td>\n",
       "      <td>0.956007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105000</td>\n",
       "      <td>0.989900</td>\n",
       "      <td>0.938850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107500</td>\n",
       "      <td>0.982500</td>\n",
       "      <td>0.947393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110000</td>\n",
       "      <td>0.976000</td>\n",
       "      <td>0.935749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112500</td>\n",
       "      <td>0.964700</td>\n",
       "      <td>0.928100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115000</td>\n",
       "      <td>0.955200</td>\n",
       "      <td>0.917355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117500</td>\n",
       "      <td>0.950300</td>\n",
       "      <td>0.921281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120000</td>\n",
       "      <td>0.945000</td>\n",
       "      <td>0.916455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122500</td>\n",
       "      <td>0.934100</td>\n",
       "      <td>0.912183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125000</td>\n",
       "      <td>0.925200</td>\n",
       "      <td>0.907297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127500</td>\n",
       "      <td>0.922800</td>\n",
       "      <td>0.896970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130000</td>\n",
       "      <td>0.916300</td>\n",
       "      <td>0.891497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132500</td>\n",
       "      <td>0.906200</td>\n",
       "      <td>0.894001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135000</td>\n",
       "      <td>0.903500</td>\n",
       "      <td>0.891968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137500</td>\n",
       "      <td>0.889000</td>\n",
       "      <td>0.887123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140000</td>\n",
       "      <td>0.890100</td>\n",
       "      <td>0.879638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142500</td>\n",
       "      <td>0.883700</td>\n",
       "      <td>0.872933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145000</td>\n",
       "      <td>0.876900</td>\n",
       "      <td>0.873646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147500</td>\n",
       "      <td>0.870300</td>\n",
       "      <td>0.871473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150000</td>\n",
       "      <td>0.865800</td>\n",
       "      <td>0.864207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152500</td>\n",
       "      <td>0.859200</td>\n",
       "      <td>0.862186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155000</td>\n",
       "      <td>0.856000</td>\n",
       "      <td>0.864353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157500</td>\n",
       "      <td>0.849400</td>\n",
       "      <td>0.859847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160000</td>\n",
       "      <td>0.845300</td>\n",
       "      <td>0.849545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162500</td>\n",
       "      <td>0.836300</td>\n",
       "      <td>0.850471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165000</td>\n",
       "      <td>0.837100</td>\n",
       "      <td>0.848610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167500</td>\n",
       "      <td>0.826000</td>\n",
       "      <td>0.851024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170000</td>\n",
       "      <td>0.823900</td>\n",
       "      <td>0.850776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172500</td>\n",
       "      <td>0.821000</td>\n",
       "      <td>0.840361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175000</td>\n",
       "      <td>0.815800</td>\n",
       "      <td>0.833989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177500</td>\n",
       "      <td>0.815900</td>\n",
       "      <td>0.839322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180000</td>\n",
       "      <td>0.809300</td>\n",
       "      <td>0.838064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182500</td>\n",
       "      <td>0.801300</td>\n",
       "      <td>0.835930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185000</td>\n",
       "      <td>0.802800</td>\n",
       "      <td>0.831192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187500</td>\n",
       "      <td>0.794300</td>\n",
       "      <td>0.833730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190000</td>\n",
       "      <td>0.791400</td>\n",
       "      <td>0.829476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192500</td>\n",
       "      <td>0.790200</td>\n",
       "      <td>0.830781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195000</td>\n",
       "      <td>0.785200</td>\n",
       "      <td>0.817905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>197500</td>\n",
       "      <td>0.778400</td>\n",
       "      <td>0.820682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200000</td>\n",
       "      <td>0.779800</td>\n",
       "      <td>0.819617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>202500</td>\n",
       "      <td>0.777800</td>\n",
       "      <td>0.815381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205000</td>\n",
       "      <td>0.766300</td>\n",
       "      <td>0.820688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>207500</td>\n",
       "      <td>0.764600</td>\n",
       "      <td>0.818502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210000</td>\n",
       "      <td>0.765600</td>\n",
       "      <td>0.813373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>212500</td>\n",
       "      <td>0.759500</td>\n",
       "      <td>0.806060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>215000</td>\n",
       "      <td>0.759300</td>\n",
       "      <td>0.812544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>217500</td>\n",
       "      <td>0.753200</td>\n",
       "      <td>0.812472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220000</td>\n",
       "      <td>0.753400</td>\n",
       "      <td>0.803495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>222500</td>\n",
       "      <td>0.748600</td>\n",
       "      <td>0.812793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225000</td>\n",
       "      <td>0.743800</td>\n",
       "      <td>0.800680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>227500</td>\n",
       "      <td>0.743400</td>\n",
       "      <td>0.802320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230000</td>\n",
       "      <td>0.739500</td>\n",
       "      <td>0.800593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>232500</td>\n",
       "      <td>0.735200</td>\n",
       "      <td>0.798331</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to roberta_lm_lm\\checkpoint-10000\n",
      "Configuration saved in roberta_lm_lm\\checkpoint-10000\\config.json\n",
      "Model weights saved in roberta_lm_lm\\checkpoint-10000\\pytorch_model.bin\n",
      "tokenizer config file saved in roberta_lm_lm\\checkpoint-10000\\tokenizer_config.json\n",
      "Special tokens file saved in roberta_lm_lm\\checkpoint-10000\\special_tokens_map.json\n",
      "Deleting older checkpoint [roberta_lm_lm\\checkpoint-340000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to roberta_lm_lm\\checkpoint-20000\n",
      "Configuration saved in roberta_lm_lm\\checkpoint-20000\\config.json\n",
      "Model weights saved in roberta_lm_lm\\checkpoint-20000\\pytorch_model.bin\n",
      "tokenizer config file saved in roberta_lm_lm\\checkpoint-20000\\tokenizer_config.json\n",
      "Special tokens file saved in roberta_lm_lm\\checkpoint-20000\\special_tokens_map.json\n",
      "Deleting older checkpoint [roberta_lm_lm\\checkpoint-350000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to roberta_lm_lm\\checkpoint-30000\n",
      "Configuration saved in roberta_lm_lm\\checkpoint-30000\\config.json\n",
      "Model weights saved in roberta_lm_lm\\checkpoint-30000\\pytorch_model.bin\n",
      "tokenizer config file saved in roberta_lm_lm\\checkpoint-30000\\tokenizer_config.json\n",
      "Special tokens file saved in roberta_lm_lm\\checkpoint-30000\\special_tokens_map.json\n",
      "Deleting older checkpoint [roberta_lm_lm\\checkpoint-360000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to roberta_lm_lm\\checkpoint-40000\n",
      "Configuration saved in roberta_lm_lm\\checkpoint-40000\\config.json\n",
      "Model weights saved in roberta_lm_lm\\checkpoint-40000\\pytorch_model.bin\n",
      "tokenizer config file saved in roberta_lm_lm\\checkpoint-40000\\tokenizer_config.json\n",
      "Special tokens file saved in roberta_lm_lm\\checkpoint-40000\\special_tokens_map.json\n",
      "Deleting older checkpoint [roberta_lm_lm\\checkpoint-10000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to roberta_lm_lm\\checkpoint-50000\n",
      "Configuration saved in roberta_lm_lm\\checkpoint-50000\\config.json\n",
      "Model weights saved in roberta_lm_lm\\checkpoint-50000\\pytorch_model.bin\n",
      "tokenizer config file saved in roberta_lm_lm\\checkpoint-50000\\tokenizer_config.json\n",
      "Special tokens file saved in roberta_lm_lm\\checkpoint-50000\\special_tokens_map.json\n",
      "Deleting older checkpoint [roberta_lm_lm\\checkpoint-20000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to roberta_lm_lm\\checkpoint-60000\n",
      "Configuration saved in roberta_lm_lm\\checkpoint-60000\\config.json\n",
      "Model weights saved in roberta_lm_lm\\checkpoint-60000\\pytorch_model.bin\n",
      "tokenizer config file saved in roberta_lm_lm\\checkpoint-60000\\tokenizer_config.json\n",
      "Special tokens file saved in roberta_lm_lm\\checkpoint-60000\\special_tokens_map.json\n",
      "Deleting older checkpoint [roberta_lm_lm\\checkpoint-30000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to roberta_lm_lm\\checkpoint-70000\n",
      "Configuration saved in roberta_lm_lm\\checkpoint-70000\\config.json\n",
      "Model weights saved in roberta_lm_lm\\checkpoint-70000\\pytorch_model.bin\n",
      "tokenizer config file saved in roberta_lm_lm\\checkpoint-70000\\tokenizer_config.json\n",
      "Special tokens file saved in roberta_lm_lm\\checkpoint-70000\\special_tokens_map.json\n",
      "Deleting older checkpoint [roberta_lm_lm\\checkpoint-40000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to roberta_lm_lm\\checkpoint-80000\n",
      "Configuration saved in roberta_lm_lm\\checkpoint-80000\\config.json\n",
      "Model weights saved in roberta_lm_lm\\checkpoint-80000\\pytorch_model.bin\n",
      "tokenizer config file saved in roberta_lm_lm\\checkpoint-80000\\tokenizer_config.json\n",
      "Special tokens file saved in roberta_lm_lm\\checkpoint-80000\\special_tokens_map.json\n",
      "Deleting older checkpoint [roberta_lm_lm\\checkpoint-50000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to roberta_lm_lm\\checkpoint-90000\n",
      "Configuration saved in roberta_lm_lm\\checkpoint-90000\\config.json\n",
      "Model weights saved in roberta_lm_lm\\checkpoint-90000\\pytorch_model.bin\n",
      "tokenizer config file saved in roberta_lm_lm\\checkpoint-90000\\tokenizer_config.json\n",
      "Special tokens file saved in roberta_lm_lm\\checkpoint-90000\\special_tokens_map.json\n",
      "Deleting older checkpoint [roberta_lm_lm\\checkpoint-60000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to roberta_lm_lm\\checkpoint-100000\n",
      "Configuration saved in roberta_lm_lm\\checkpoint-100000\\config.json\n",
      "Model weights saved in roberta_lm_lm\\checkpoint-100000\\pytorch_model.bin\n",
      "tokenizer config file saved in roberta_lm_lm\\checkpoint-100000\\tokenizer_config.json\n",
      "Special tokens file saved in roberta_lm_lm\\checkpoint-100000\\special_tokens_map.json\n",
      "Deleting older checkpoint [roberta_lm_lm\\checkpoint-70000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to roberta_lm_lm\\checkpoint-110000\n",
      "Configuration saved in roberta_lm_lm\\checkpoint-110000\\config.json\n",
      "Model weights saved in roberta_lm_lm\\checkpoint-110000\\pytorch_model.bin\n",
      "tokenizer config file saved in roberta_lm_lm\\checkpoint-110000\\tokenizer_config.json\n",
      "Special tokens file saved in roberta_lm_lm\\checkpoint-110000\\special_tokens_map.json\n",
      "Deleting older checkpoint [roberta_lm_lm\\checkpoint-80000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to roberta_lm_lm\\checkpoint-120000\n",
      "Configuration saved in roberta_lm_lm\\checkpoint-120000\\config.json\n",
      "Model weights saved in roberta_lm_lm\\checkpoint-120000\\pytorch_model.bin\n",
      "tokenizer config file saved in roberta_lm_lm\\checkpoint-120000\\tokenizer_config.json\n",
      "Special tokens file saved in roberta_lm_lm\\checkpoint-120000\\special_tokens_map.json\n",
      "Deleting older checkpoint [roberta_lm_lm\\checkpoint-90000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to roberta_lm_lm\\checkpoint-130000\n",
      "Configuration saved in roberta_lm_lm\\checkpoint-130000\\config.json\n",
      "Model weights saved in roberta_lm_lm\\checkpoint-130000\\pytorch_model.bin\n",
      "tokenizer config file saved in roberta_lm_lm\\checkpoint-130000\\tokenizer_config.json\n",
      "Special tokens file saved in roberta_lm_lm\\checkpoint-130000\\special_tokens_map.json\n",
      "Deleting older checkpoint [roberta_lm_lm\\checkpoint-100000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to roberta_lm_lm\\checkpoint-140000\n",
      "Configuration saved in roberta_lm_lm\\checkpoint-140000\\config.json\n",
      "Model weights saved in roberta_lm_lm\\checkpoint-140000\\pytorch_model.bin\n",
      "tokenizer config file saved in roberta_lm_lm\\checkpoint-140000\\tokenizer_config.json\n",
      "Special tokens file saved in roberta_lm_lm\\checkpoint-140000\\special_tokens_map.json\n",
      "Deleting older checkpoint [roberta_lm_lm\\checkpoint-110000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to roberta_lm_lm\\checkpoint-150000\n",
      "Configuration saved in roberta_lm_lm\\checkpoint-150000\\config.json\n",
      "Model weights saved in roberta_lm_lm\\checkpoint-150000\\pytorch_model.bin\n",
      "tokenizer config file saved in roberta_lm_lm\\checkpoint-150000\\tokenizer_config.json\n",
      "Special tokens file saved in roberta_lm_lm\\checkpoint-150000\\special_tokens_map.json\n",
      "Deleting older checkpoint [roberta_lm_lm\\checkpoint-120000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to roberta_lm_lm\\checkpoint-160000\n",
      "Configuration saved in roberta_lm_lm\\checkpoint-160000\\config.json\n",
      "Model weights saved in roberta_lm_lm\\checkpoint-160000\\pytorch_model.bin\n",
      "tokenizer config file saved in roberta_lm_lm\\checkpoint-160000\\tokenizer_config.json\n",
      "Special tokens file saved in roberta_lm_lm\\checkpoint-160000\\special_tokens_map.json\n",
      "Deleting older checkpoint [roberta_lm_lm\\checkpoint-130000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to roberta_lm_lm\\checkpoint-170000\n",
      "Configuration saved in roberta_lm_lm\\checkpoint-170000\\config.json\n",
      "Model weights saved in roberta_lm_lm\\checkpoint-170000\\pytorch_model.bin\n",
      "tokenizer config file saved in roberta_lm_lm\\checkpoint-170000\\tokenizer_config.json\n",
      "Special tokens file saved in roberta_lm_lm\\checkpoint-170000\\special_tokens_map.json\n",
      "Deleting older checkpoint [roberta_lm_lm\\checkpoint-140000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to roberta_lm_lm\\checkpoint-180000\n",
      "Configuration saved in roberta_lm_lm\\checkpoint-180000\\config.json\n",
      "Model weights saved in roberta_lm_lm\\checkpoint-180000\\pytorch_model.bin\n",
      "tokenizer config file saved in roberta_lm_lm\\checkpoint-180000\\tokenizer_config.json\n",
      "Special tokens file saved in roberta_lm_lm\\checkpoint-180000\\special_tokens_map.json\n",
      "Deleting older checkpoint [roberta_lm_lm\\checkpoint-150000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to roberta_lm_lm\\checkpoint-190000\n",
      "Configuration saved in roberta_lm_lm\\checkpoint-190000\\config.json\n",
      "Model weights saved in roberta_lm_lm\\checkpoint-190000\\pytorch_model.bin\n",
      "tokenizer config file saved in roberta_lm_lm\\checkpoint-190000\\tokenizer_config.json\n",
      "Special tokens file saved in roberta_lm_lm\\checkpoint-190000\\special_tokens_map.json\n",
      "Deleting older checkpoint [roberta_lm_lm\\checkpoint-160000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to roberta_lm_lm\\checkpoint-200000\n",
      "Configuration saved in roberta_lm_lm\\checkpoint-200000\\config.json\n",
      "Model weights saved in roberta_lm_lm\\checkpoint-200000\\pytorch_model.bin\n",
      "tokenizer config file saved in roberta_lm_lm\\checkpoint-200000\\tokenizer_config.json\n",
      "Special tokens file saved in roberta_lm_lm\\checkpoint-200000\\special_tokens_map.json\n",
      "Deleting older checkpoint [roberta_lm_lm\\checkpoint-170000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to roberta_lm_lm\\checkpoint-210000\n",
      "Configuration saved in roberta_lm_lm\\checkpoint-210000\\config.json\n",
      "Model weights saved in roberta_lm_lm\\checkpoint-210000\\pytorch_model.bin\n",
      "tokenizer config file saved in roberta_lm_lm\\checkpoint-210000\\tokenizer_config.json\n",
      "Special tokens file saved in roberta_lm_lm\\checkpoint-210000\\special_tokens_map.json\n",
      "Deleting older checkpoint [roberta_lm_lm\\checkpoint-180000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to roberta_lm_lm\\checkpoint-220000\n",
      "Configuration saved in roberta_lm_lm\\checkpoint-220000\\config.json\n",
      "Model weights saved in roberta_lm_lm\\checkpoint-220000\\pytorch_model.bin\n",
      "tokenizer config file saved in roberta_lm_lm\\checkpoint-220000\\tokenizer_config.json\n",
      "Special tokens file saved in roberta_lm_lm\\checkpoint-220000\\special_tokens_map.json\n",
      "Deleting older checkpoint [roberta_lm_lm\\checkpoint-190000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to roberta_lm_lm\\checkpoint-230000\n",
      "Configuration saved in roberta_lm_lm\\checkpoint-230000\\config.json\n",
      "Model weights saved in roberta_lm_lm\\checkpoint-230000\\pytorch_model.bin\n",
      "tokenizer config file saved in roberta_lm_lm\\checkpoint-230000\\tokenizer_config.json\n",
      "Special tokens file saved in roberta_lm_lm\\checkpoint-230000\\special_tokens_map.json\n",
      "Deleting older checkpoint [roberta_lm_lm\\checkpoint-200000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3427\n",
      "  Batch size = 24\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 6858\n",
      "  Batch size = 24\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='286' max='286' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [286/286 00:48]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.8642981648445129,\n",
       " 'eval_runtime': 49.1668,\n",
       " 'eval_samples_per_second': 139.484,\n",
       " 'eval_steps_per_second': 5.817,\n",
       " 'epoch': 180.0}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_output = trainer.evaluate(tokenized_datasets[\"test\"]); eval_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to roberta_lm_lm\n",
      "Configuration saved in roberta_lm_lm\\config.json\n",
      "Model weights saved in roberta_lm_lm\\pytorch_model.bin\n",
      "tokenizer config file saved in roberta_lm_lm\\tokenizer_config.json\n",
      "Special tokens file saved in roberta_lm_lm\\special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.373339808006104\n"
     ]
    }
   ],
   "source": [
    "perplexity = math.exp(eval_output[\"eval_loss\"])\n",
    "print(perplexity) #2.756616"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file roberta_lm_lm\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 16000\n",
      "}\n",
      "\n",
      "loading configuration file roberta_lm_lm\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 16000\n",
      "}\n",
      "\n",
      "loading weights file roberta_lm_lm\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForMaskedLM.\n",
      "\n",
      "All the weights of RobertaForMaskedLM were initialized from the model checkpoint at roberta_lm_lm.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.\n",
      "Didn't find file roberta_lm_lm\\added_tokens.json. We won't load it.\n",
      "loading file roberta_lm_lm\\vocab.json\n",
      "loading file roberta_lm_lm\\merges.txt\n",
      "loading file roberta_lm_lm\\tokenizer.json\n",
      "loading file None\n",
      "loading file roberta_lm_lm\\special_tokens_map.json\n",
      "loading file roberta_lm_lm\\tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "fill_mask = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=notebook_path_prefix+\"_lm\",\n",
    "    tokenizer=notebook_path_prefix+\"_lm\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': 'Hello II.',\n",
       "  'score': 0.10519658774137497,\n",
       "  'token': 1064,\n",
       "  'token_str': ' II'},\n",
       " {'sequence': 'Hello Commission.',\n",
       "  'score': 0.05819733068346977,\n",
       "  'token': 484,\n",
       "  'token_str': ' Commission'},\n",
       " {'sequence': 'Hellotices.',\n",
       "  'score': 0.05515122041106224,\n",
       "  'token': 2233,\n",
       "  'token_str': 'tices'},\n",
       " {'sequence': 'Hello speech.',\n",
       "  'score': 0.02830634079873562,\n",
       "  'token': 15420,\n",
       "  'token_str': ' speech'},\n",
       " {'sequence': 'Hello all.',\n",
       "  'score': 0.02700468897819519,\n",
       "  'token': 673,\n",
       "  'token_str': ' all'}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask(\"Hello <mask>.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
