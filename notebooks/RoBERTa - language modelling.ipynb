{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RoBERTa LM training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "wandb: Currently logged in as: bmarcin (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 16_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = [\n",
    "    '<url>',\n",
    "    '<email>',\n",
    "    '<number>',\n",
    "    '<date>', \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_ds = \"../data/dev/lm.txt\"\n",
    "test_ds = \"../data/test/lm.txt\"\n",
    "train_ds = \"../data/train/lm.txt\"\n",
    "\n",
    "notebook_path_prefix = \"roberta_lm\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe = ByteLevelBPETokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe.train(\n",
    "    files=[train_ds], \n",
    "    vocab_size=vocab_size, \n",
    "    min_frequency=2, \n",
    "    special_tokens=[\n",
    "        \"<s>\",\n",
    "        \"<pad>\",\n",
    "        \"</s>\",\n",
    "        \"<unk>\",\n",
    "        \"<mask>\",\n",
    "    ] + special_tokens\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['roberta_lm\\\\vocab.json', 'roberta_lm\\\\merges.txt']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.makedirs(notebook_path_prefix, exist_ok=True)\n",
    "bpe.save_model(notebook_path_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig, RobertaTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "file roberta_lm\\config.json not found\n",
      "file roberta_lm\\config.json not found\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizerFast.from_pretrained(notebook_path_prefix, max_len=512, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_special_tokens({\n",
    "    'additional_special_tokens': special_tokens\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<s>',\n",
       " 'eos_token': '</s>',\n",
       " 'unk_token': '<unk>',\n",
       " 'sep_token': '</s>',\n",
       " 'pad_token': '<pad>',\n",
       " 'cls_token': '<s>',\n",
       " 'mask_token': '<mask>',\n",
       " 'additional_special_tokens': ['<url>', '<email>', '<number>', '<date>']}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-3c47a6e81fa8bdd0\n",
      "Reusing dataset text (C:\\Users\\Marcin Borzymowski\\.cache\\huggingface\\datasets\\text\\default-3c47a6e81fa8bdd0\\0.0.0\\e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d346d4ff5dd458791d22b1d2b9b687f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset('text', data_files={'train': [train_ds], 'test': [test_ds], 'dev': [dev_ds]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\Marcin Borzymowski\\.cache\\huggingface\\datasets\\text\\default-3c47a6e81fa8bdd0\\0.0.0\\e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5\\cache-903a4b9c797dc1db.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Marcin Borzymowski\\.cache\\huggingface\\datasets\\text\\default-3c47a6e81fa8bdd0\\0.0.0\\e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5\\cache-c09a14bed6e863ab.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Marcin Borzymowski\\.cache\\huggingface\\datasets\\text\\default-3c47a6e81fa8bdd0\\0.0.0\\e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5\\cache-bccb1c343ded8d36.arrow\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = dataset.map(\n",
    "    encode,\n",
    "    batched=True,\n",
    "    remove_columns=['text'],\n",
    "    load_from_cache_file=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DS collocator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = RobertaConfig(\n",
    "    vocab_size=vocab_size,\n",
    "    max_position_embeddings=514,\n",
    "    num_attention_heads=8,\n",
    "    num_hidden_layers=6,\n",
    "    type_vocab_size=1,\n",
    "    layer_norm_eps=0.00001,\n",
    "    hidden_size=512,\n",
    "    hidden_dropout_prob=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RobertaForMaskedLM(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33948288"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=notebook_path_prefix+\"_lm\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=180,\n",
    "    per_device_train_batch_size=12,\n",
    "    per_device_eval_batch_size=24,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=3,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    no_cuda=False,\n",
    "    logging_steps=2500,\n",
    "    eval_steps=2500,\n",
    "    evaluation_strategy='steps',\n",
    "    report_to=\"wandb\",\n",
    "    run_name=\"roberta-lm\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets['dev']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 24003\n",
      "  Num Epochs = 180\n",
      "  Instantaneous batch size per device = 12\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 360180\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "wandb: wandb version 0.12.6 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/bmarcin/huggingface/runs/i1fxei83\" target=\"_blank\">roberta-lm</a></strong> to <a href=\"https://wandb.ai/bmarcin/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='360180' max='360180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [360180/360180 28:06:35, Epoch 180/180]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>5.834300</td>\n",
       "      <td>5.292369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>5.175000</td>\n",
       "      <td>4.762578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>4.504600</td>\n",
       "      <td>3.654576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>3.747500</td>\n",
       "      <td>3.104758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>3.287900</td>\n",
       "      <td>2.722188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>2.914800</td>\n",
       "      <td>2.410820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>2.618000</td>\n",
       "      <td>2.189506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>2.404500</td>\n",
       "      <td>2.019231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>2.211100</td>\n",
       "      <td>1.853542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>2.049400</td>\n",
       "      <td>1.704045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>1.905200</td>\n",
       "      <td>1.599305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>1.794500</td>\n",
       "      <td>1.523222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>1.704900</td>\n",
       "      <td>1.468250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>1.637700</td>\n",
       "      <td>1.395632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37500</td>\n",
       "      <td>1.575300</td>\n",
       "      <td>1.362361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>1.524600</td>\n",
       "      <td>1.313195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42500</td>\n",
       "      <td>1.474700</td>\n",
       "      <td>1.284076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>1.434400</td>\n",
       "      <td>1.255409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47500</td>\n",
       "      <td>1.399900</td>\n",
       "      <td>1.222704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>1.367600</td>\n",
       "      <td>1.202198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52500</td>\n",
       "      <td>1.336100</td>\n",
       "      <td>1.187209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55000</td>\n",
       "      <td>1.308400</td>\n",
       "      <td>1.157568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57500</td>\n",
       "      <td>1.273500</td>\n",
       "      <td>1.141198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>1.259700</td>\n",
       "      <td>1.120837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62500</td>\n",
       "      <td>1.232200</td>\n",
       "      <td>1.113339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65000</td>\n",
       "      <td>1.217200</td>\n",
       "      <td>1.092631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67500</td>\n",
       "      <td>1.194900</td>\n",
       "      <td>1.078555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>1.175800</td>\n",
       "      <td>1.066059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72500</td>\n",
       "      <td>1.155200</td>\n",
       "      <td>1.055512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75000</td>\n",
       "      <td>1.142500</td>\n",
       "      <td>1.048039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77500</td>\n",
       "      <td>1.125700</td>\n",
       "      <td>1.035574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80000</td>\n",
       "      <td>1.114200</td>\n",
       "      <td>1.029491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82500</td>\n",
       "      <td>1.094500</td>\n",
       "      <td>1.015799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85000</td>\n",
       "      <td>1.083800</td>\n",
       "      <td>1.008690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87500</td>\n",
       "      <td>1.074800</td>\n",
       "      <td>1.004656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90000</td>\n",
       "      <td>1.059700</td>\n",
       "      <td>0.987819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92500</td>\n",
       "      <td>1.043500</td>\n",
       "      <td>0.989203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95000</td>\n",
       "      <td>1.034000</td>\n",
       "      <td>0.973058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97500</td>\n",
       "      <td>1.028900</td>\n",
       "      <td>0.967375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100000</td>\n",
       "      <td>1.016000</td>\n",
       "      <td>0.965456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102500</td>\n",
       "      <td>1.005200</td>\n",
       "      <td>0.952177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105000</td>\n",
       "      <td>0.998500</td>\n",
       "      <td>0.949660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107500</td>\n",
       "      <td>0.980900</td>\n",
       "      <td>0.942269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110000</td>\n",
       "      <td>0.975600</td>\n",
       "      <td>0.932933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112500</td>\n",
       "      <td>0.969600</td>\n",
       "      <td>0.928932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115000</td>\n",
       "      <td>0.957600</td>\n",
       "      <td>0.927569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117500</td>\n",
       "      <td>0.951000</td>\n",
       "      <td>0.922772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120000</td>\n",
       "      <td>0.943600</td>\n",
       "      <td>0.914538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122500</td>\n",
       "      <td>0.936600</td>\n",
       "      <td>0.911353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125000</td>\n",
       "      <td>0.928500</td>\n",
       "      <td>0.910166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127500</td>\n",
       "      <td>0.920700</td>\n",
       "      <td>0.906101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130000</td>\n",
       "      <td>0.917500</td>\n",
       "      <td>0.905418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132500</td>\n",
       "      <td>0.905200</td>\n",
       "      <td>0.886639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135000</td>\n",
       "      <td>0.903800</td>\n",
       "      <td>0.894227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137500</td>\n",
       "      <td>0.894900</td>\n",
       "      <td>0.891238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140000</td>\n",
       "      <td>0.887200</td>\n",
       "      <td>0.883640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142500</td>\n",
       "      <td>0.883500</td>\n",
       "      <td>0.872744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145000</td>\n",
       "      <td>0.876900</td>\n",
       "      <td>0.877487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147500</td>\n",
       "      <td>0.872400</td>\n",
       "      <td>0.870913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150000</td>\n",
       "      <td>0.866800</td>\n",
       "      <td>0.867776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152500</td>\n",
       "      <td>0.858300</td>\n",
       "      <td>0.868498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155000</td>\n",
       "      <td>0.852600</td>\n",
       "      <td>0.857059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157500</td>\n",
       "      <td>0.853100</td>\n",
       "      <td>0.856063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160000</td>\n",
       "      <td>0.843400</td>\n",
       "      <td>0.853153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162500</td>\n",
       "      <td>0.836700</td>\n",
       "      <td>0.865853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165000</td>\n",
       "      <td>0.835700</td>\n",
       "      <td>0.854120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167500</td>\n",
       "      <td>0.828900</td>\n",
       "      <td>0.852120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170000</td>\n",
       "      <td>0.828200</td>\n",
       "      <td>0.842182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172500</td>\n",
       "      <td>0.818600</td>\n",
       "      <td>0.839677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175000</td>\n",
       "      <td>0.818600</td>\n",
       "      <td>0.848679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177500</td>\n",
       "      <td>0.810500</td>\n",
       "      <td>0.837419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180000</td>\n",
       "      <td>0.811000</td>\n",
       "      <td>0.838113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182500</td>\n",
       "      <td>0.802700</td>\n",
       "      <td>0.834487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185000</td>\n",
       "      <td>0.797500</td>\n",
       "      <td>0.829406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187500</td>\n",
       "      <td>0.799000</td>\n",
       "      <td>0.830832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190000</td>\n",
       "      <td>0.791900</td>\n",
       "      <td>0.825978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192500</td>\n",
       "      <td>0.785400</td>\n",
       "      <td>0.827486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195000</td>\n",
       "      <td>0.782700</td>\n",
       "      <td>0.826774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>197500</td>\n",
       "      <td>0.779400</td>\n",
       "      <td>0.819193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200000</td>\n",
       "      <td>0.777300</td>\n",
       "      <td>0.822262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>202500</td>\n",
       "      <td>0.773800</td>\n",
       "      <td>0.822614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205000</td>\n",
       "      <td>0.768100</td>\n",
       "      <td>0.820587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>207500</td>\n",
       "      <td>0.765200</td>\n",
       "      <td>0.821637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210000</td>\n",
       "      <td>0.761400</td>\n",
       "      <td>0.815177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>212500</td>\n",
       "      <td>0.759600</td>\n",
       "      <td>0.814430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>215000</td>\n",
       "      <td>0.756500</td>\n",
       "      <td>0.813513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>217500</td>\n",
       "      <td>0.752400</td>\n",
       "      <td>0.804342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220000</td>\n",
       "      <td>0.750600</td>\n",
       "      <td>0.808731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>222500</td>\n",
       "      <td>0.746100</td>\n",
       "      <td>0.804194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225000</td>\n",
       "      <td>0.743200</td>\n",
       "      <td>0.803177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>227500</td>\n",
       "      <td>0.740200</td>\n",
       "      <td>0.798442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230000</td>\n",
       "      <td>0.737900</td>\n",
       "      <td>0.801582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>232500</td>\n",
       "      <td>0.737900</td>\n",
       "      <td>0.798369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>235000</td>\n",
       "      <td>0.728900</td>\n",
       "      <td>0.799759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>237500</td>\n",
       "      <td>0.728800</td>\n",
       "      <td>0.792128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240000</td>\n",
       "      <td>0.732500</td>\n",
       "      <td>0.795528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>242500</td>\n",
       "      <td>0.722500</td>\n",
       "      <td>0.792948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>245000</td>\n",
       "      <td>0.719600</td>\n",
       "      <td>0.796106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>247500</td>\n",
       "      <td>0.718200</td>\n",
       "      <td>0.790331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250000</td>\n",
       "      <td>0.719800</td>\n",
       "      <td>0.789924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>252500</td>\n",
       "      <td>0.711400</td>\n",
       "      <td>0.789617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>255000</td>\n",
       "      <td>0.713600</td>\n",
       "      <td>0.786350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>257500</td>\n",
       "      <td>0.708800</td>\n",
       "      <td>0.788815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260000</td>\n",
       "      <td>0.706100</td>\n",
       "      <td>0.784530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>262500</td>\n",
       "      <td>0.704700</td>\n",
       "      <td>0.787944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>265000</td>\n",
       "      <td>0.700100</td>\n",
       "      <td>0.786384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>267500</td>\n",
       "      <td>0.697800</td>\n",
       "      <td>0.785677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270000</td>\n",
       "      <td>0.699000</td>\n",
       "      <td>0.787368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>272500</td>\n",
       "      <td>0.694500</td>\n",
       "      <td>0.785821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275000</td>\n",
       "      <td>0.695000</td>\n",
       "      <td>0.783430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>277500</td>\n",
       "      <td>0.691400</td>\n",
       "      <td>0.777859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280000</td>\n",
       "      <td>0.688200</td>\n",
       "      <td>0.782934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>282500</td>\n",
       "      <td>0.690000</td>\n",
       "      <td>0.777858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>285000</td>\n",
       "      <td>0.684600</td>\n",
       "      <td>0.782208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>287500</td>\n",
       "      <td>0.686200</td>\n",
       "      <td>0.777826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290000</td>\n",
       "      <td>0.680200</td>\n",
       "      <td>0.773256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>292500</td>\n",
       "      <td>0.682300</td>\n",
       "      <td>0.773710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>295000</td>\n",
       "      <td>0.679000</td>\n",
       "      <td>0.773300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>297500</td>\n",
       "      <td>0.679000</td>\n",
       "      <td>0.775747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300000</td>\n",
       "      <td>0.676200</td>\n",
       "      <td>0.768661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>302500</td>\n",
       "      <td>0.672300</td>\n",
       "      <td>0.773946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>305000</td>\n",
       "      <td>0.676200</td>\n",
       "      <td>0.776643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>307500</td>\n",
       "      <td>0.669200</td>\n",
       "      <td>0.763056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310000</td>\n",
       "      <td>0.671700</td>\n",
       "      <td>0.765232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>312500</td>\n",
       "      <td>0.668200</td>\n",
       "      <td>0.769164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>315000</td>\n",
       "      <td>0.666100</td>\n",
       "      <td>0.763693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>317500</td>\n",
       "      <td>0.667300</td>\n",
       "      <td>0.769068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320000</td>\n",
       "      <td>0.666300</td>\n",
       "      <td>0.773726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>322500</td>\n",
       "      <td>0.663400</td>\n",
       "      <td>0.767399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325000</td>\n",
       "      <td>0.661700</td>\n",
       "      <td>0.759311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>327500</td>\n",
       "      <td>0.656500</td>\n",
       "      <td>0.755059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330000</td>\n",
       "      <td>0.661000</td>\n",
       "      <td>0.767170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>332500</td>\n",
       "      <td>0.657400</td>\n",
       "      <td>0.767121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>335000</td>\n",
       "      <td>0.658900</td>\n",
       "      <td>0.765166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>337500</td>\n",
       "      <td>0.655800</td>\n",
       "      <td>0.758591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340000</td>\n",
       "      <td>0.652900</td>\n",
       "      <td>0.759185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>342500</td>\n",
       "      <td>0.653400</td>\n",
       "      <td>0.761688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>345000</td>\n",
       "      <td>0.655700</td>\n",
       "      <td>0.764197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>347500</td>\n",
       "      <td>0.655600</td>\n",
       "      <td>0.763510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350000</td>\n",
       "      <td>0.651200</td>\n",
       "      <td>0.760218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>352500</td>\n",
       "      <td>0.651900</td>\n",
       "      <td>0.760437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>355000</td>\n",
       "      <td>0.648800</td>\n",
       "      <td>0.753538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>357500</td>\n",
       "      <td>0.650900</td>\n",
       "      <td>0.757636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360000</td>\n",
       "      <td>0.651100</td>\n",
       "      <td>0.763372</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to roberta_lm_lm\\checkpoint-10000\n",
      "Configuration saved in roberta_lm_lm\\checkpoint-10000\\config.json\n",
      "Model weights saved in roberta_lm_lm\\checkpoint-10000\\pytorch_model.bin\n",
      "tokenizer config file saved in roberta_lm_lm\\checkpoint-10000\\tokenizer_config.json\n",
      "Special tokens file saved in roberta_lm_lm\\checkpoint-10000\\special_tokens_map.json\n",
      "Deleting older checkpoint [roberta_lm_lm\\checkpoint-160000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to roberta_lm_lm\\checkpoint-20000\n",
      "Configuration saved in roberta_lm_lm\\checkpoint-20000\\config.json\n",
      "Model weights saved in roberta_lm_lm\\checkpoint-20000\\pytorch_model.bin\n",
      "tokenizer config file saved in roberta_lm_lm\\checkpoint-20000\\tokenizer_config.json\n",
      "Special tokens file saved in roberta_lm_lm\\checkpoint-20000\\special_tokens_map.json\n",
      "Deleting older checkpoint [roberta_lm_lm\\checkpoint-170000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to roberta_lm_lm\\checkpoint-30000\n",
      "Configuration saved in roberta_lm_lm\\checkpoint-30000\\config.json\n",
      "Model weights saved in roberta_lm_lm\\checkpoint-30000\\pytorch_model.bin\n",
      "tokenizer config file saved in roberta_lm_lm\\checkpoint-30000\\tokenizer_config.json\n",
      "Special tokens file saved in roberta_lm_lm\\checkpoint-30000\\special_tokens_map.json\n",
      "Deleting older checkpoint [roberta_lm_lm\\checkpoint-180000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to roberta_lm_lm\\checkpoint-40000\n",
      "Configuration saved in roberta_lm_lm\\checkpoint-40000\\config.json\n",
      "Model weights saved in roberta_lm_lm\\checkpoint-40000\\pytorch_model.bin\n",
      "tokenizer config file saved in roberta_lm_lm\\checkpoint-40000\\tokenizer_config.json\n",
      "Special tokens file saved in roberta_lm_lm\\checkpoint-40000\\special_tokens_map.json\n",
      "Deleting older checkpoint [roberta_lm_lm\\checkpoint-10000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to roberta_lm_lm\\checkpoint-50000\n",
      "Configuration saved in roberta_lm_lm\\checkpoint-50000\\config.json\n",
      "Model weights saved in roberta_lm_lm\\checkpoint-50000\\pytorch_model.bin\n",
      "tokenizer config file saved in roberta_lm_lm\\checkpoint-50000\\tokenizer_config.json\n",
      "Special tokens file saved in roberta_lm_lm\\checkpoint-50000\\special_tokens_map.json\n",
      "Deleting older checkpoint [roberta_lm_lm\\checkpoint-20000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to roberta_lm_lm\\checkpoint-60000\n",
      "Configuration saved in roberta_lm_lm\\checkpoint-60000\\config.json\n",
      "Model weights saved in roberta_lm_lm\\checkpoint-60000\\pytorch_model.bin\n",
      "tokenizer config file saved in roberta_lm_lm\\checkpoint-60000\\tokenizer_config.json\n",
      "Special tokens file saved in roberta_lm_lm\\checkpoint-60000\\special_tokens_map.json\n",
      "Deleting older checkpoint [roberta_lm_lm\\checkpoint-30000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to roberta_lm_lm\\checkpoint-70000\n",
      "Configuration saved in roberta_lm_lm\\checkpoint-70000\\config.json\n",
      "Model weights saved in roberta_lm_lm\\checkpoint-70000\\pytorch_model.bin\n",
      "tokenizer config file saved in roberta_lm_lm\\checkpoint-70000\\tokenizer_config.json\n",
      "Special tokens file saved in roberta_lm_lm\\checkpoint-70000\\special_tokens_map.json\n",
      "Deleting older checkpoint [roberta_lm_lm\\checkpoint-40000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to roberta_lm_lm\\checkpoint-80000\n",
      "Configuration saved in roberta_lm_lm\\checkpoint-80000\\config.json\n",
      "Model weights saved in roberta_lm_lm\\checkpoint-80000\\pytorch_model.bin\n",
      "tokenizer config file saved in roberta_lm_lm\\checkpoint-80000\\tokenizer_config.json\n",
      "Special tokens file saved in roberta_lm_lm\\checkpoint-80000\\special_tokens_map.json\n",
      "Deleting older checkpoint [roberta_lm_lm\\checkpoint-50000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to roberta_lm_lm\\checkpoint-90000\n",
      "Configuration saved in roberta_lm_lm\\checkpoint-90000\\config.json\n",
      "Model weights saved in roberta_lm_lm\\checkpoint-90000\\pytorch_model.bin\n",
      "tokenizer config file saved in roberta_lm_lm\\checkpoint-90000\\tokenizer_config.json\n",
      "Special tokens file saved in roberta_lm_lm\\checkpoint-90000\\special_tokens_map.json\n",
      "Deleting older checkpoint [roberta_lm_lm\\checkpoint-60000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to roberta_lm_lm\\checkpoint-100000\n",
      "Configuration saved in roberta_lm_lm\\checkpoint-100000\\config.json\n",
      "Model weights saved in roberta_lm_lm\\checkpoint-100000\\pytorch_model.bin\n",
      "tokenizer config file saved in roberta_lm_lm\\checkpoint-100000\\tokenizer_config.json\n",
      "Special tokens file saved in roberta_lm_lm\\checkpoint-100000\\special_tokens_map.json\n",
      "Deleting older checkpoint [roberta_lm_lm\\checkpoint-70000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to roberta_lm_lm\\checkpoint-110000\n",
      "Configuration saved in roberta_lm_lm\\checkpoint-110000\\config.json\n",
      "Model weights saved in roberta_lm_lm\\checkpoint-110000\\pytorch_model.bin\n",
      "tokenizer config file saved in roberta_lm_lm\\checkpoint-110000\\tokenizer_config.json\n",
      "Special tokens file saved in roberta_lm_lm\\checkpoint-110000\\special_tokens_map.json\n",
      "Deleting older checkpoint [roberta_lm_lm\\checkpoint-80000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to roberta_lm_lm\\checkpoint-120000\n",
      "Configuration saved in roberta_lm_lm\\checkpoint-120000\\config.json\n",
      "Model weights saved in roberta_lm_lm\\checkpoint-120000\\pytorch_model.bin\n",
      "tokenizer config file saved in roberta_lm_lm\\checkpoint-120000\\tokenizer_config.json\n",
      "Special tokens file saved in roberta_lm_lm\\checkpoint-120000\\special_tokens_map.json\n",
      "Deleting older checkpoint [roberta_lm_lm\\checkpoint-90000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to roberta_lm_lm\\checkpoint-130000\n",
      "Configuration saved in roberta_lm_lm\\checkpoint-130000\\config.json\n",
      "Model weights saved in roberta_lm_lm\\checkpoint-130000\\pytorch_model.bin\n",
      "tokenizer config file saved in roberta_lm_lm\\checkpoint-130000\\tokenizer_config.json\n",
      "Special tokens file saved in roberta_lm_lm\\checkpoint-130000\\special_tokens_map.json\n",
      "Deleting older checkpoint [roberta_lm_lm\\checkpoint-100000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to roberta_lm_lm\\checkpoint-140000\n",
      "Configuration saved in roberta_lm_lm\\checkpoint-140000\\config.json\n",
      "Model weights saved in roberta_lm_lm\\checkpoint-140000\\pytorch_model.bin\n",
      "tokenizer config file saved in roberta_lm_lm\\checkpoint-140000\\tokenizer_config.json\n",
      "Special tokens file saved in roberta_lm_lm\\checkpoint-140000\\special_tokens_map.json\n",
      "Deleting older checkpoint [roberta_lm_lm\\checkpoint-110000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to roberta_lm_lm\\checkpoint-150000\n",
      "Configuration saved in roberta_lm_lm\\checkpoint-150000\\config.json\n",
      "Model weights saved in roberta_lm_lm\\checkpoint-150000\\pytorch_model.bin\n",
      "tokenizer config file saved in roberta_lm_lm\\checkpoint-150000\\tokenizer_config.json\n",
      "Special tokens file saved in roberta_lm_lm\\checkpoint-150000\\special_tokens_map.json\n",
      "Deleting older checkpoint [roberta_lm_lm\\checkpoint-120000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to roberta_lm_lm\\checkpoint-160000\n",
      "Configuration saved in roberta_lm_lm\\checkpoint-160000\\config.json\n",
      "Model weights saved in roberta_lm_lm\\checkpoint-160000\\pytorch_model.bin\n",
      "tokenizer config file saved in roberta_lm_lm\\checkpoint-160000\\tokenizer_config.json\n",
      "Special tokens file saved in roberta_lm_lm\\checkpoint-160000\\special_tokens_map.json\n",
      "Deleting older checkpoint [roberta_lm_lm\\checkpoint-130000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to roberta_lm_lm\\checkpoint-170000\n",
      "Configuration saved in roberta_lm_lm\\checkpoint-170000\\config.json\n",
      "Model weights saved in roberta_lm_lm\\checkpoint-170000\\pytorch_model.bin\n",
      "tokenizer config file saved in roberta_lm_lm\\checkpoint-170000\\tokenizer_config.json\n",
      "Special tokens file saved in roberta_lm_lm\\checkpoint-170000\\special_tokens_map.json\n",
      "Deleting older checkpoint [roberta_lm_lm\\checkpoint-140000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to roberta_lm_lm\\checkpoint-180000\n",
      "Configuration saved in roberta_lm_lm\\checkpoint-180000\\config.json\n",
      "Model weights saved in roberta_lm_lm\\checkpoint-180000\\pytorch_model.bin\n",
      "tokenizer config file saved in roberta_lm_lm\\checkpoint-180000\\tokenizer_config.json\n",
      "Special tokens file saved in roberta_lm_lm\\checkpoint-180000\\special_tokens_map.json\n",
      "Deleting older checkpoint [roberta_lm_lm\\checkpoint-150000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to roberta_lm_lm\\checkpoint-190000\n",
      "Configuration saved in roberta_lm_lm\\checkpoint-190000\\config.json\n",
      "Model weights saved in roberta_lm_lm\\checkpoint-190000\\pytorch_model.bin\n",
      "tokenizer config file saved in roberta_lm_lm\\checkpoint-190000\\tokenizer_config.json\n",
      "Special tokens file saved in roberta_lm_lm\\checkpoint-190000\\special_tokens_map.json\n",
      "Deleting older checkpoint [roberta_lm_lm\\checkpoint-160000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to roberta_lm_lm\\checkpoint-200000\n",
      "Configuration saved in roberta_lm_lm\\checkpoint-200000\\config.json\n",
      "Model weights saved in roberta_lm_lm\\checkpoint-200000\\pytorch_model.bin\n",
      "tokenizer config file saved in roberta_lm_lm\\checkpoint-200000\\tokenizer_config.json\n",
      "Special tokens file saved in roberta_lm_lm\\checkpoint-200000\\special_tokens_map.json\n",
      "Deleting older checkpoint [roberta_lm_lm\\checkpoint-170000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to roberta_lm_lm\\checkpoint-210000\n",
      "Configuration saved in roberta_lm_lm\\checkpoint-210000\\config.json\n",
      "Model weights saved in roberta_lm_lm\\checkpoint-210000\\pytorch_model.bin\n",
      "tokenizer config file saved in roberta_lm_lm\\checkpoint-210000\\tokenizer_config.json\n",
      "Special tokens file saved in roberta_lm_lm\\checkpoint-210000\\special_tokens_map.json\n",
      "Deleting older checkpoint [roberta_lm_lm\\checkpoint-180000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to roberta_lm_lm\\checkpoint-220000\n",
      "Configuration saved in roberta_lm_lm\\checkpoint-220000\\config.json\n",
      "Model weights saved in roberta_lm_lm\\checkpoint-220000\\pytorch_model.bin\n",
      "tokenizer config file saved in roberta_lm_lm\\checkpoint-220000\\tokenizer_config.json\n",
      "Special tokens file saved in roberta_lm_lm\\checkpoint-220000\\special_tokens_map.json\n",
      "Deleting older checkpoint [roberta_lm_lm\\checkpoint-190000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to roberta_lm_lm\\checkpoint-230000\n",
      "Configuration saved in roberta_lm_lm\\checkpoint-230000\\config.json\n",
      "Model weights saved in roberta_lm_lm\\checkpoint-230000\\pytorch_model.bin\n",
      "tokenizer config file saved in roberta_lm_lm\\checkpoint-230000\\tokenizer_config.json\n",
      "Special tokens file saved in roberta_lm_lm\\checkpoint-230000\\special_tokens_map.json\n",
      "Deleting older checkpoint [roberta_lm_lm\\checkpoint-200000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to roberta_lm_lm\\checkpoint-240000\n",
      "Configuration saved in roberta_lm_lm\\checkpoint-240000\\config.json\n",
      "Model weights saved in roberta_lm_lm\\checkpoint-240000\\pytorch_model.bin\n",
      "tokenizer config file saved in roberta_lm_lm\\checkpoint-240000\\tokenizer_config.json\n",
      "Special tokens file saved in roberta_lm_lm\\checkpoint-240000\\special_tokens_map.json\n",
      "Deleting older checkpoint [roberta_lm_lm\\checkpoint-210000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to roberta_lm_lm\\checkpoint-250000\n",
      "Configuration saved in roberta_lm_lm\\checkpoint-250000\\config.json\n",
      "Model weights saved in roberta_lm_lm\\checkpoint-250000\\pytorch_model.bin\n",
      "tokenizer config file saved in roberta_lm_lm\\checkpoint-250000\\tokenizer_config.json\n",
      "Special tokens file saved in roberta_lm_lm\\checkpoint-250000\\special_tokens_map.json\n",
      "Deleting older checkpoint [roberta_lm_lm\\checkpoint-220000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to roberta_lm_lm\\checkpoint-260000\n",
      "Configuration saved in roberta_lm_lm\\checkpoint-260000\\config.json\n",
      "Model weights saved in roberta_lm_lm\\checkpoint-260000\\pytorch_model.bin\n",
      "tokenizer config file saved in roberta_lm_lm\\checkpoint-260000\\tokenizer_config.json\n",
      "Special tokens file saved in roberta_lm_lm\\checkpoint-260000\\special_tokens_map.json\n",
      "Deleting older checkpoint [roberta_lm_lm\\checkpoint-230000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to roberta_lm_lm\\checkpoint-270000\n",
      "Configuration saved in roberta_lm_lm\\checkpoint-270000\\config.json\n",
      "Model weights saved in roberta_lm_lm\\checkpoint-270000\\pytorch_model.bin\n",
      "tokenizer config file saved in roberta_lm_lm\\checkpoint-270000\\tokenizer_config.json\n",
      "Special tokens file saved in roberta_lm_lm\\checkpoint-270000\\special_tokens_map.json\n",
      "Deleting older checkpoint [roberta_lm_lm\\checkpoint-240000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to roberta_lm_lm\\checkpoint-280000\n",
      "Configuration saved in roberta_lm_lm\\checkpoint-280000\\config.json\n",
      "Model weights saved in roberta_lm_lm\\checkpoint-280000\\pytorch_model.bin\n",
      "tokenizer config file saved in roberta_lm_lm\\checkpoint-280000\\tokenizer_config.json\n",
      "Special tokens file saved in roberta_lm_lm\\checkpoint-280000\\special_tokens_map.json\n",
      "Deleting older checkpoint [roberta_lm_lm\\checkpoint-250000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to roberta_lm_lm\\checkpoint-290000\n",
      "Configuration saved in roberta_lm_lm\\checkpoint-290000\\config.json\n",
      "Model weights saved in roberta_lm_lm\\checkpoint-290000\\pytorch_model.bin\n",
      "tokenizer config file saved in roberta_lm_lm\\checkpoint-290000\\tokenizer_config.json\n",
      "Special tokens file saved in roberta_lm_lm\\checkpoint-290000\\special_tokens_map.json\n",
      "Deleting older checkpoint [roberta_lm_lm\\checkpoint-260000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to roberta_lm_lm\\checkpoint-300000\n",
      "Configuration saved in roberta_lm_lm\\checkpoint-300000\\config.json\n",
      "Model weights saved in roberta_lm_lm\\checkpoint-300000\\pytorch_model.bin\n",
      "tokenizer config file saved in roberta_lm_lm\\checkpoint-300000\\tokenizer_config.json\n",
      "Special tokens file saved in roberta_lm_lm\\checkpoint-300000\\special_tokens_map.json\n",
      "Deleting older checkpoint [roberta_lm_lm\\checkpoint-270000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to roberta_lm_lm\\checkpoint-310000\n",
      "Configuration saved in roberta_lm_lm\\checkpoint-310000\\config.json\n",
      "Model weights saved in roberta_lm_lm\\checkpoint-310000\\pytorch_model.bin\n",
      "tokenizer config file saved in roberta_lm_lm\\checkpoint-310000\\tokenizer_config.json\n",
      "Special tokens file saved in roberta_lm_lm\\checkpoint-310000\\special_tokens_map.json\n",
      "Deleting older checkpoint [roberta_lm_lm\\checkpoint-280000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to roberta_lm_lm\\checkpoint-320000\n",
      "Configuration saved in roberta_lm_lm\\checkpoint-320000\\config.json\n",
      "Model weights saved in roberta_lm_lm\\checkpoint-320000\\pytorch_model.bin\n",
      "tokenizer config file saved in roberta_lm_lm\\checkpoint-320000\\tokenizer_config.json\n",
      "Special tokens file saved in roberta_lm_lm\\checkpoint-320000\\special_tokens_map.json\n",
      "Deleting older checkpoint [roberta_lm_lm\\checkpoint-290000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to roberta_lm_lm\\checkpoint-330000\n",
      "Configuration saved in roberta_lm_lm\\checkpoint-330000\\config.json\n",
      "Model weights saved in roberta_lm_lm\\checkpoint-330000\\pytorch_model.bin\n",
      "tokenizer config file saved in roberta_lm_lm\\checkpoint-330000\\tokenizer_config.json\n",
      "Special tokens file saved in roberta_lm_lm\\checkpoint-330000\\special_tokens_map.json\n",
      "Deleting older checkpoint [roberta_lm_lm\\checkpoint-300000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to roberta_lm_lm\\checkpoint-340000\n",
      "Configuration saved in roberta_lm_lm\\checkpoint-340000\\config.json\n",
      "Model weights saved in roberta_lm_lm\\checkpoint-340000\\pytorch_model.bin\n",
      "tokenizer config file saved in roberta_lm_lm\\checkpoint-340000\\tokenizer_config.json\n",
      "Special tokens file saved in roberta_lm_lm\\checkpoint-340000\\special_tokens_map.json\n",
      "Deleting older checkpoint [roberta_lm_lm\\checkpoint-310000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to roberta_lm_lm\\checkpoint-350000\n",
      "Configuration saved in roberta_lm_lm\\checkpoint-350000\\config.json\n",
      "Model weights saved in roberta_lm_lm\\checkpoint-350000\\pytorch_model.bin\n",
      "tokenizer config file saved in roberta_lm_lm\\checkpoint-350000\\tokenizer_config.json\n",
      "Special tokens file saved in roberta_lm_lm\\checkpoint-350000\\special_tokens_map.json\n",
      "Deleting older checkpoint [roberta_lm_lm\\checkpoint-320000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3429\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to roberta_lm_lm\\checkpoint-360000\n",
      "Configuration saved in roberta_lm_lm\\checkpoint-360000\\config.json\n",
      "Model weights saved in roberta_lm_lm\\checkpoint-360000\\pytorch_model.bin\n",
      "tokenizer config file saved in roberta_lm_lm\\checkpoint-360000\\tokenizer_config.json\n",
      "Special tokens file saved in roberta_lm_lm\\checkpoint-360000\\special_tokens_map.json\n",
      "Deleting older checkpoint [roberta_lm_lm\\checkpoint-330000] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=360180, training_loss=1.0622865363701477, metrics={'train_runtime': 101201.1935, 'train_samples_per_second': 42.693, 'train_steps_per_second': 3.559, 'total_flos': 3.3835570964987904e+17, 'train_loss': 1.0622865363701477, 'epoch': 180.0})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 6858\n",
      "  Batch size = 24\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='286' max='286' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [286/286 00:48]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.8642981648445129,\n",
       " 'eval_runtime': 49.1668,\n",
       " 'eval_samples_per_second': 139.484,\n",
       " 'eval_steps_per_second': 5.817,\n",
       " 'epoch': 180.0}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_output = trainer.evaluate(tokenized_datasets[\"test\"]); eval_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to roberta_lm_lm\n",
      "Configuration saved in roberta_lm_lm\\config.json\n",
      "Model weights saved in roberta_lm_lm\\pytorch_model.bin\n",
      "tokenizer config file saved in roberta_lm_lm\\tokenizer_config.json\n",
      "Special tokens file saved in roberta_lm_lm\\special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.373339808006104\n"
     ]
    }
   ],
   "source": [
    "perplexity = math.exp(eval_output[\"eval_loss\"])\n",
    "print(perplexity) #2.756616"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file roberta_lm_lm\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 16000\n",
      "}\n",
      "\n",
      "loading configuration file roberta_lm_lm\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 16000\n",
      "}\n",
      "\n",
      "loading weights file roberta_lm_lm\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForMaskedLM.\n",
      "\n",
      "All the weights of RobertaForMaskedLM were initialized from the model checkpoint at roberta_lm_lm.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.\n",
      "Didn't find file roberta_lm_lm\\added_tokens.json. We won't load it.\n",
      "loading file roberta_lm_lm\\vocab.json\n",
      "loading file roberta_lm_lm\\merges.txt\n",
      "loading file roberta_lm_lm\\tokenizer.json\n",
      "loading file None\n",
      "loading file roberta_lm_lm\\special_tokens_map.json\n",
      "loading file roberta_lm_lm\\tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "fill_mask = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=notebook_path_prefix+\"_lm\",\n",
    "    tokenizer=notebook_path_prefix+\"_lm\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': 'Hello II.',\n",
       "  'score': 0.10519658774137497,\n",
       "  'token': 1064,\n",
       "  'token_str': ' II'},\n",
       " {'sequence': 'Hello Commission.',\n",
       "  'score': 0.05819733068346977,\n",
       "  'token': 484,\n",
       "  'token_str': ' Commission'},\n",
       " {'sequence': 'Hellotices.',\n",
       "  'score': 0.05515122041106224,\n",
       "  'token': 2233,\n",
       "  'token_str': 'tices'},\n",
       " {'sequence': 'Hello speech.',\n",
       "  'score': 0.02830634079873562,\n",
       "  'token': 15420,\n",
       "  'token_str': ' speech'},\n",
       " {'sequence': 'Hello all.',\n",
       "  'score': 0.02700468897819519,\n",
       "  'token': 673,\n",
       "  'token_str': ' all'}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask(\"Hello <mask>.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
