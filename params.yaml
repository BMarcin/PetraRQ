datasetsplit:
  train: 0.85
  test: 0.1
  dev: 0.05
  parts: 3
  sort_by: date #date or hash

datasetrewrite:
  threads: 64

language_modeling_train:
  cuda_visible_devices: [0, 1, 2, 3]

  vocab_size: 16000
  tokenizer_min_frequency: 2
  max_seq_length: 512
  mlm_probability: 0.15

  num_attention_heads: 12
  num_hidden_layers: 12
  hidden_size: 768
  hidden_dropout_prob: 0.1

  epochs: 200
  per_device_train_batch_size: 26
  per_device_eval_batch_size: 40

  seed: 42

classification_train:
  cuda_visible_devices: [0, 1, 2, 3]

  warmup_steps: 500
  num_train_epochs: 20
  num_training_samples: 100

  per_device_train_batch_size: 30
  per_device_eval_batch_size: 40

  outputs: probabilities #labels or probabilities
  seed: 42

classification_eval:
  epsilon: 1e-2
