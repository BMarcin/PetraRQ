datasetsplit:
  train: 0.85
  test: 0.1
  dev: 0.05
  parts: 3
  sort_by: date #date or hash

datasetrewrite:
  threads: 64

language_modeling_train:
  cuda_visible_devices: [0, 1, 2]

  vocab_size: 16000
  tokenizer_min_frequency: 2
  max_seq_length: 512
  mlm_probability: 0.15

  num_attention_heads: 12
  num_hidden_layers: 12
  hidden_size: 768
  hidden_dropout_prob: 0.1

  epochs: 200
  per_device_train_batch_size: 26
  per_device_eval_batch_size: 40

  seed: 42

classification_train:
  cuda_visible_devices: [5]

  steps: 5000
  num_training_samples: 1000
  seq_length: 512
  overlapping_part: 256
  lr: 1e-3
  increase_each: 0.05

  train_batch_size: 32
  dev_batch_size: 30

  outputs: probabilities #labels or probabilities
  seed: 42
  use_wandb_logging: true

classification_eval:
  cuda_visible_devices: [5]
  epsilon: 1e-2
